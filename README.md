# RESIDENCIAS-FER-PEDRO
Sistema de Detecci√≥n de Anomal√≠as de Red (Prototipo PoC)Este repositorio contiene el c√≥digo de una Prueba de Concepto (PoC) para un sistema de detecci√≥n de anomal√≠as en tiempo real. El objetivo principal es validar la arquitectura y la integraci√≥n de un stack tecnol√≥gico moderno (Docker, TIG Stack) para el procesamiento y visualizaci√≥n de datos de alta frecuencia.‚ö†Ô∏è ¬°Importante! Prop√≥sito de esta Prueba de ConceptoEste proyecto NO es un sistema de detecci√≥n de intrusos listo para producci√≥n. Su prop√≥sito fue estrictamente t√©cnico:Validar las Herramientas: Probar la viabilidad de usar Docker, Prometheus, InfluxDB y Grafana juntos.Validar el Pipeline de Datos: Asegurar que un script de Python puede capturar datos, procesarlos con un modelo de Keras y enviarlos a dos bases de datos (de m√©tricas y de series temporales) en paralelo.Probar el Despliegue: Demostrar la facilidad de despliegue y replicaci√≥n del entorno completo usando Docker Compose.El modelo de Deep Learning (tu_modelo_lstm.h5) es un placeholder entrenado con datos aleatorios (generador_modelo_prueba.py) solo para validar el flujo de inferencia.üèõÔ∏è Arquitectura del Sistema (PoC)El prototipo implementa un flujo de datos de doble v√≠a para m√©tricas en tiempo real y almacenamiento hist√≥rico.Captura: Un contenedor capturador con Python y Scapy escucha el tr√°fico de red del host.Procesamiento: Cada paquete es analizado, sus caracter√≠sticas son extra√≠das y preprocesadas (usando mi_scaler.joblib), y se calcula un score de anomal√≠a (Error de Reconstrucci√≥n del Autoencoder) usando el modelo de Keras (tu_modelo_lstm.h5).Almacenamiento (V√≠a 1 - Tiempo Real): El script expone m√©tricas instant√°neas (ej. anomalia_de_red_score) en un endpoint (:8000). Prometheus recolecta (scrapes) estas m√©tricas peri√≥dicamente.Almacenamiento (V√≠a 2 - Hist√≥rico): Los datos detallados de cada paquete (IPs, puertos, score) se escriben en InfluxDB para an√°lisis a largo plazo.Visualizaci√≥n: Grafana se conecta a ambas fuentes de datos (Prometheus y InfluxDB) para mostrarlas en un dashboard unificado.üöÄ Stack Tecnol√≥gicoOrquestaci√≥n: Docker & Docker ComposeCaptura y Modelo (Servicio capturador):Python 3.12Scapy: Para la captura de paquetes de red.TensorFlow / Keras: Para cargar y ejecutar el modelo de inferencia.Scikit-Learn: Para cargar el scaler de preprocesamiento.Pandas / Numpy: Para la manipulaci√≥n de datos.Base de Datos (Series Temporales): InfluxDB 2.xMonitoreo y M√©tricas: PrometheusVisualizaci√≥n: Grafanaüîß Gu√≠a de Despliegue R√°pidoEl uso de Docker Compose hace que el despliegue en una nueva m√°quina sea trivial, siempre que los archivos del modelo ya existan.PrerrequisitosDockerDocker Compose (Versi√≥n 2, el comando es docker compose)Sistema operativo Linux (para network_mode: "host")1. Generar el Modelo de Prueba (Paso √∫nico)Si no tienes los archivos tu_modelo_lstm.h5 y mi_scaler.joblib, debes generarlos primero:# 1. Crear y activar un entorno virtual
python3 -m venv venv
source venv/bin/activate

# 2. Instalar dependencias locales
pip install -r requirements.txt

# 3. Ejecutar el script generador
python generador_modelo_prueba.py

# 4. Desactivar el entorno (opcional)
deactivate
2. Levantar el Stack CompletoCon todos los archivos del proyecto en el directorio (incluyendo el .h5 y .joblib), ejecuta:# Construye las im√°genes y levanta los contenedores
# -d (detached) los ejecuta en segundo plano
sudo docker compose up --build -d
3. Verificar el EstadoPara asegurarte de que todos los servicios est√°n corriendo:sudo docker compose ps
Deber√≠as ver los 4 contenedores (capturador, grafana, influxdb, prometheus) con el estado Up.üñ•Ô∏è Acceso a los ServiciosUna vez levantado el stack, puedes acceder a las interfaces web desde la m√°quina host:Grafana (Visualizaci√≥n):URL: http://localhost:3000User: adminPass: admin (te pedir√° cambiarla)Prometheus (M√©tricas):URL: http://localhost:9090Para ver el estado del capturador: Status -> TargetsInfluxDB (Base de Datos):URL: http://localhost:8086User: my-userPass: my-super-passwordüîÆ Pr√≥ximos Pasos (Visi√≥n del Proyecto Final)Este prototipo valid√≥ la arquitectura. El proyecto final evolucionar√° de la siguiente manera:Fuente de Datos: Se reemplazar√° Scapy por un sistema de ingesta de logs de servidor web (ej. Nginx, Apache).Modelo Especializado: Se entrenar√° un nuevo modelo con un dataset especializado en la detecci√≥n de patrones de ataque en logs web (ej. Inyecci√≥n SQL, XSS, Scaneo de directorios).Despliegue: La arquitectura de Docker Compose se desplegar√° en un servidor de producci√≥n para monitorear el servidor web en vivo.
